{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Coursera - Word Embedding with Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "\n",
      "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "\n",
      "Padded Sequences:\n",
      "[[ 0  5  3  2  4]\n",
      " [ 0  5  3  2  7]\n",
      " [ 0  6  3  2  4]\n",
      " [ 9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "padded = pad_sequences(sequences, maxlen=5)\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "print(\"\\nSequences = \" , sequences)\n",
    "print(\"\\nPadded Sequences:\")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n",
      "\n",
      "Padded Test Sequence: \n",
      "[[5 1 3 2 4]\n",
      " [2 4 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "padded = pad_sequences(test_seq, maxlen=5)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarcase Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 20000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dataset/sarcasm.json\", 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "\n",
    "sentences = [] \n",
    "labels = []\n",
    "urls = []\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x\n",
    "import numpy as np\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "625/625 - 1s - loss: 0.6533 - accuracy: 0.6011 - val_loss: 0.5422 - val_accuracy: 0.8111\n",
      "Epoch 2/30\n",
      "625/625 - 1s - loss: 0.4074 - accuracy: 0.8425 - val_loss: 0.3782 - val_accuracy: 0.8430\n",
      "Epoch 3/30\n",
      "625/625 - 1s - loss: 0.3018 - accuracy: 0.8812 - val_loss: 0.3497 - val_accuracy: 0.8550\n",
      "Epoch 4/30\n",
      "625/625 - 1s - loss: 0.2498 - accuracy: 0.9027 - val_loss: 0.3565 - val_accuracy: 0.8474\n",
      "Epoch 5/30\n",
      "625/625 - 1s - loss: 0.2144 - accuracy: 0.9186 - val_loss: 0.3466 - val_accuracy: 0.8562\n",
      "Epoch 6/30\n",
      "625/625 - 1s - loss: 0.1895 - accuracy: 0.9283 - val_loss: 0.3654 - val_accuracy: 0.8509\n",
      "Epoch 7/30\n",
      "625/625 - 1s - loss: 0.1688 - accuracy: 0.9381 - val_loss: 0.3721 - val_accuracy: 0.8511\n",
      "Epoch 8/30\n",
      "625/625 - 1s - loss: 0.1500 - accuracy: 0.9461 - val_loss: 0.3827 - val_accuracy: 0.8542\n",
      "Epoch 9/30\n",
      "625/625 - 1s - loss: 0.1356 - accuracy: 0.9515 - val_loss: 0.4039 - val_accuracy: 0.8506\n",
      "Epoch 10/30\n",
      "625/625 - 1s - loss: 0.1228 - accuracy: 0.9576 - val_loss: 0.4259 - val_accuracy: 0.8480\n",
      "Epoch 11/30\n",
      "625/625 - 1s - loss: 0.1110 - accuracy: 0.9624 - val_loss: 0.4576 - val_accuracy: 0.8405\n",
      "Epoch 12/30\n",
      "625/625 - 1s - loss: 0.1014 - accuracy: 0.9660 - val_loss: 0.5083 - val_accuracy: 0.8325\n",
      "Epoch 13/30\n",
      "625/625 - 1s - loss: 0.0925 - accuracy: 0.9689 - val_loss: 0.5112 - val_accuracy: 0.8377\n",
      "Epoch 14/30\n",
      "625/625 - 1s - loss: 0.0846 - accuracy: 0.9724 - val_loss: 0.5292 - val_accuracy: 0.8380\n",
      "Epoch 15/30\n",
      "625/625 - 1s - loss: 0.0776 - accuracy: 0.9754 - val_loss: 0.5814 - val_accuracy: 0.8317\n",
      "Epoch 16/30\n",
      "625/625 - 1s - loss: 0.0719 - accuracy: 0.9773 - val_loss: 0.6470 - val_accuracy: 0.8271\n",
      "Epoch 17/30\n",
      "625/625 - 1s - loss: 0.0661 - accuracy: 0.9782 - val_loss: 0.6221 - val_accuracy: 0.8314\n",
      "Epoch 18/30\n",
      "625/625 - 1s - loss: 0.0600 - accuracy: 0.9818 - val_loss: 0.6515 - val_accuracy: 0.8286\n",
      "Epoch 19/30\n",
      "625/625 - 1s - loss: 0.0537 - accuracy: 0.9844 - val_loss: 0.7141 - val_accuracy: 0.8261\n",
      "Epoch 20/30\n",
      "625/625 - 1s - loss: 0.0523 - accuracy: 0.9842 - val_loss: 0.7175 - val_accuracy: 0.8220\n",
      "Epoch 21/30\n",
      "625/625 - 1s - loss: 0.0470 - accuracy: 0.9862 - val_loss: 0.7827 - val_accuracy: 0.8217\n",
      "Epoch 22/30\n",
      "625/625 - 1s - loss: 0.0441 - accuracy: 0.9874 - val_loss: 0.8262 - val_accuracy: 0.8196\n",
      "Epoch 23/30\n",
      "625/625 - 1s - loss: 0.0399 - accuracy: 0.9887 - val_loss: 0.8903 - val_accuracy: 0.8171\n",
      "Epoch 24/30\n",
      "625/625 - 1s - loss: 0.0382 - accuracy: 0.9890 - val_loss: 0.8763 - val_accuracy: 0.8179\n",
      "Epoch 25/30\n",
      "625/625 - 1s - loss: 0.0323 - accuracy: 0.9913 - val_loss: 0.9535 - val_accuracy: 0.8150\n",
      "Epoch 26/30\n",
      "625/625 - 1s - loss: 0.0298 - accuracy: 0.9919 - val_loss: 0.9402 - val_accuracy: 0.8153\n",
      "Epoch 27/30\n",
      "625/625 - 1s - loss: 0.0289 - accuracy: 0.9918 - val_loss: 1.0017 - val_accuracy: 0.8126\n",
      "Epoch 28/30\n",
      "625/625 - 1s - loss: 0.0262 - accuracy: 0.9929 - val_loss: 1.0309 - val_accuracy: 0.8146\n",
      "Epoch 29/30\n",
      "625/625 - 1s - loss: 0.0243 - accuracy: 0.9933 - val_loss: 1.0615 - val_accuracy: 0.8123\n",
      "Epoch 30/30\n",
      "625/625 - 1s - loss: 0.0230 - accuracy: 0.9937 - val_loss: 1.1130 - val_accuracy: 0.8141\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs,\n",
    "                    validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1feca8b3a752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
